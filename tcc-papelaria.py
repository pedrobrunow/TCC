# -*- coding: utf-8 -*-
"""analise-qualitativa-quantitativa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15CoaxBHnx5eiH1DHelBlZo9qpcIzvWTW

# Importação das bibliotecas
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

df = pd.read_csv('/content/vendas.csv', sep=';', dtype={'CEP': str})
df = df[df.columns[:17]]

# Mostra as primeiras linhas do DataFrame
print(df.head(5))

print(df.dtypes)

df.describe(include='all').T

"""# Descrição de atributos

## DOCUMENTO

Se trata do registro da venda, uma espécie de código para cada venda realizada
"""

# Calcula a quantidade de dados únicos na coluna 'DOCUMENTO'
unique_counts = df['DOCUMENTO'].nunique()

# Imprime a quantidade de dados únicos para cada coluna
print(unique_counts)

"""Como a quantidade de valores únicos (5365) foi diferentes do total de linhas (9321), depois de analisar os dados manualmente, podemos concluir que existem muitas duplicatas, que precisam ser removidas"""

df = df.drop_duplicates(subset='DOCUMENTO', keep='first')
total_linhas = df.shape[0]
print(total_linhas)

"""## COD. PRODUTO e NOME PRODUTO"""

# Verifica a consistência entre "COD. PRODUTO" e "NOME PRODUTO"
produtos_inconsistentes = df.groupby('COD. PRODUTO')['NOME PRODUTO'].nunique() > 1
if produtos_inconsistentes.any():
    print("Há inconsistências entre 'COD. PRODUTO' e 'NOME PRODUTO' nas seguintes linhas:")
    print(produtos_inconsistentes[produtos_inconsistentes])
else:
    print("Não foram encontradas inconsistências entre 'COD. PRODUTO' e 'NOME PRODUTO'.")

pivot_table = df[['COD. PRODUTO', 'NOME PRODUTO']].drop_duplicates().sort_values('COD. PRODUTO')
pivot_table

"""## NCM

Nomenclatura Comum do Mercosul. A NCM é um sistema de códigos utilizado para classificar e identificar mercadorias. Cada produto tem um código NCM específico, que é reconhecido internacionalmente para fins de cálculo de impostos e coleta de estatísticas de comércio internacional.
"""

df_grouped = df.groupby('NCM')['COD. PRODUTO'].nunique()
print(df_grouped)
df_grouped.plot(kind='bar')
plt.ylabel('Número de Produtos Únicos')
plt.show()

df_grouped = df.groupby('NCM')['NOME PRODUTO'].unique()
print(df_grouped)
for ncm, produtos in df_grouped.iteritems():
    print(f'NCM: {ncm}')
    for produto in produtos:
        print(f'    Produto: {produto}')

"""## QUANTIDADE VENDIDA"""

# Conta o número de vezes que cada produto foi vendido
product_counts = df['NOME PRODUTO'].value_counts()

# Plote um gráfico de barras com os produtos mais vendidos
plt.figure(figsize=(10, 5))  # Ajusta o tamanho da figura
sns.barplot(y=product_counts.index, x=product_counts.values, orient='h')
plt.xlabel('Número de Vendas')
plt.ylabel('Nome do Produto')
plt.title('Número de Vendas por Produto')
plt.show()

# Agrupa os dados por "Nome do Produto" e calcula a soma de "Quantidade Vendida" para cada produto
vendas_por_produto = df.groupby('NOME PRODUTO')['QUANTIDADE VENDIDA'].sum().sort_values()

# Cria um gráfico de barras da quantidade de vendas por produto
plt.figure(figsize=(10, 5))
sns.barplot(y=vendas_por_produto.index, x=vendas_por_produto.values, orient='h')
plt.xlabel('Nome do Produto')
plt.ylabel('Quantidade Vendida')
plt.title('Quantidade de Vendas por Produto')
plt.show()

# Calcular a média da quantidade vendida de cada produto
avg_quantity = df.groupby('NOME PRODUTO')['QUANTIDADE VENDIDA'].mean().sort_values()

# Plotar a média da quantidade vendida por produto
plt.figure(figsize=(10, 5))
sns.barplot(y=avg_quantity.index, x=avg_quantity.values, orient='h')
plt.xlabel('Nome do Produto')
plt.ylabel('Média da Quantidade Vendida')
plt.title('Média da Quantidade Vendida por Produto')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['QUANTIDADE VENDIDA'])
plt.title('Boxplot - Quantidade Vendida')
plt.show()

outliers = df[df['QUANTIDADE VENDIDA'] > 800]
print(outliers[['NOME CLIENTE', 'NOME PRODUTO', 'QUANTIDADE VENDIDA', 'DATA VENDA']])

"""## VALOR LÍQ. VENDA e VALOR BRUTO VENDA"""

# Substituir vírgulas por pontos
df['VALOR BRUTO VENDA'] = df['VALOR BRUTO VENDA'].str.replace(',', '.')
df['VALOR LÍQ. VENDA'] = df['VALOR LÍQ. VENDA'].str.replace(',', '.')

# Converter as colunas para float
df['VALOR BRUTO VENDA'] = df['VALOR BRUTO VENDA'].astype(float)
df['VALOR LÍQ. VENDA'] = df['VALOR LÍQ. VENDA'].astype(float)

# Encontra valores de venda abaixo de 0
vendas_negativas = df['VALOR LÍQ. VENDA'] < 0

# Verifica se há alguma venda negativa
if vendas_negativas.any():
    print("Há vendas com valor abaixo de 0 nas seguintes linhas:")
    print(df2[vendas_negativas])
else:
    print("Não há vendas com valor abaixo de 0.")

# Cria uma nova coluna 'IGUALDADE' que é True onde 'SUBTOTAL' é igual a 'VALOR LÍQ. VENDA' e False caso contrário
df['IGUALDADE'] = df['VALOR BRUTO VENDA'] == df['VALOR LÍQ. VENDA']

num_igual = df['IGUALDADE'].sum()

print(f'O valor bruto de venda é igual ao valor líquido de venda {num_igual} vezes.')

# Conta o número total de linhas no dataframe
total_linhas = len(df)

# Calcula a porcentagem
porcentagem = (num_igual / total_linhas) * 100

print(f'O valor bruto de venda é igual ao valor líquido de venda em {porcentagem}% do dataset.')

# Criar uma nova coluna que representa a diferença entre o valor bruto e líquido
df['DIFERENCA'] = df['VALOR BRUTO VENDA'] - df['VALOR LÍQ. VENDA']

plt.hist(df['DIFERENCA'], bins=30, edgecolor='black')
plt.title('Distribuição da Diferença entre Valor Bruto e Valor Líquido')
plt.xlabel('Diferença')
plt.ylabel('Frequência')
plt.show()

"""## SUBTOTAL"""

print('Média:', df['SUBTOTAL'].mean())
print('Mediana:', df['SUBTOTAL'].median())
print('Mínimo:', df['SUBTOTAL'].min())
print('Máximo:', df['SUBTOTAL'].max())
print('Desvio padrão:', df['SUBTOTAL'].std())

# Removendo possíveis símbolos de moeda e convertendo vírgulas para pontos
if df["VALOR LÍQ. VENDA"].dtype == 'O':  # Se o tipo de dados for Object (string)
    df["VALOR LÍQ. VENDA"] = df["VALOR LÍQ. VENDA"].str.replace(',', '.').str.replace('R$', '').astype(float)

# Verificando se todas as linhas nas colunas são iguais
sao_iguais = (df["SUBTOTAL"] == df["VALOR LÍQ. VENDA"]).all()

print(sao_iguais)

# Criando uma série booleana para as diferenças
diferencas = df["SUBTOTAL"] != df["VALOR LÍQ. VENDA"]

# Calculando o total de linhas diferentes
total_diferencas = diferencas.sum()

# Calculando a porcentagem
porcentagem_diferente = (total_diferencas / len(df)) * 100

print(f"{porcentagem_diferente:.2f}% dos valores são diferentes.")

"""## DATA VENDA"""

# Convertendo a coluna "Data Venda" para o tipo datetime
df['DATA VENDA'] = pd.to_datetime(df['DATA VENDA'], format='%d/%m/%Y')

# Definindo 'DATA VENDA' como índice para utilizar a funcionalidade resample
df_indexado = df.set_index('DATA VENDA')

# Agrupando por mês e somando as quantidades vendidas
quantidade_por_mes = df_indexado.resample('M')['SUBTOTAL'].sum()

# Plotando o gráfico de barras
plt.figure(figsize=(14, 7))
quantidade_por_mes.plot(kind='bar', color='skyblue')
plt.title('TOTAL DE GASTOS (R$) por Mês')
plt.xlabel('Mês')
plt.ylabel('TOTAL DE GASTOS (R$)')
plt.tight_layout()
plt.xticks(rotation=90)  # Rotaciona os rótulos do eixo X para melhor visualização
plt.grid(axis='y')
plt.show()

# Encontra a data de venda mais recente e a data de venda mais antiga de venda
data_recente = df['DATA VENDA'].max()
data_antiga = df['DATA VENDA'].min()

print("A data de venda mais recente é:", data_recente)
print("A data de venda mais antiga é:", data_antiga)

"""## CODIGO CLIENTE"""

# Calcula a quantidade de dados únicos nas colunas 'COD. CLIENTE'
unique_counts = df['COD. CLIENTE'].nunique()

# Imprime a quantidade de dados únicos para cada coluna
print(unique_counts)

"""## NOME CLIENTE"""

df['NOME CLIENTE'] = df['NOME CLIENTE'].str.upper()

# Calcula a quantidade de dados únicos nas colunas 'NOME CLIENTE'
unique_counts = df['NOME CLIENTE'].nunique()

# Imprime a quantidade de dados únicos para cada coluna
print(unique_counts)

clientes_inconsistentes = df.groupby('COD. CLIENTE')['NOME CLIENTE'].nunique() > 1
if clientes_inconsistentes.any():
    print("Há inconsistências entre 'COD. CLIENTE' e 'NOME CLIENTE' nas seguintes linhas:")
    print(clientes_inconsistentes[clientes_inconsistentes])
else:
    print("Não foram encontradas inconsistências entre 'COD. CLIENTE' e 'NOME CLIENTE'.")

"""Notamos que a contagem de valores únicos nas colunas 'COD. CLIENTE' e 'NOME CLIENTE' está diferente, mesmo após verificar a consistência.

## CNPJ/CPF
"""

# Verifica se o formato do CNPJ/CPF está correto
cpf_cnpj_formato_correto = df['CNPJ/CPF'].str.contains(r'\d{3}\.\d{3}\.\d{3}-\d{2}|\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}')
if not cpf_cnpj_formato_correto.all():
    print("Os seguintes CNPJ/CPF não estão no formato correto:")
    print(df2.loc[~cpf_cnpj_formato_correto, 'CNPJ/CPF'])

clientes_inconsistentes = df.groupby('COD. CLIENTE')['CNPJ/CPF'].nunique() > 1
if clientes_inconsistentes.any():
    print("Há inconsistências entre 'COD. CLIENTE' e 'CNPJ/CPF' nas seguintes linhas:")
    print(clientes_inconsistentes[clientes_inconsistentes])
else:
    print("Não foram encontradas inconsistências entre 'COD. CLIENTE' e 'CNPJ/CPF'.")

df['PESSOA/EMPRESA'] = np.where(df['CNPJ/CPF'].str.len() == 14, 1, 2)

# Calculando o número de pessoas físicas e empresas
num_pessoas_fisicas = df[df['PESSOA/EMPRESA'] == 1].shape[0]
num_empresas = df[df['PESSOA/EMPRESA'] == 2].shape[0]

print(f"Número de Pessoas Físicas: {num_pessoas_fisicas}")
print(f"Número de Empresas: {num_empresas}")

"""## CEP"""

# Garantindo que todos os CEPs estejam no formato correto
df['CEP'] = df['CEP'].str.replace('-', '').str.zfill(8)
#df['CEP'] = df['CEP'].str[:5] + '-' + df['CEP'].str[5:]
print(df['CEP'])

import requests

# Cria um dicionário vazio para armazenar as informações de cada CEP
cep_info = {}

# Percorre cada CEP único no conjunto de dados
for cep in df['CEP'].unique():
    # Se as informações desse CEP já foram buscadas, pula para o próximo
    if cep in cep_info:
        continue

    # Busca as informações do CEP usando a API do ViaCEP
    response = requests.get(f'https://viacep.com.br/ws/{cep}/json/')

    # Se a resposta for bem-sucedida, armazena as informações no dicionário
    if response.status_code == 200:
        cep_info[cep] = response.json()

# Percorre cada linha do conjunto de dados
for i, row in df.iterrows():
    # Verifica se o CEP existe no dicionário cep_info
    if row['CEP'] in cep_info:
        # Obtém as informações do CEP para essa linha
        info = cep_info[row['CEP']]

        # Corrige os dados de bairro, cidade e estado usando as informações do CEP
        # Verifica se os campos existem antes de acessá-los
        if 'bairro' in info:
            df.at[i, 'BAIRRO'] = info['bairro']
        if 'localidade' in info:
            df.at[i, 'CIDADE'] = info['localidade']
        if 'uf' in info:
            df.at[i, 'UF'] = info['uf']
    else:
        print(f"CEP {row['CEP']} não encontrado no dicionário cep_info.")

# Conta o número de CEPs faltantes
missing_ceps = df['CEP'].apply(lambda x: x not in cep_info).sum()
print(f"Há {missing_ceps} CEPs faltantes.")
# Remove as linhas com CEPs faltantes
df = df[df['CEP'].apply(lambda x: x in cep_info)]

print("Linhas com CEPs faltantes foram removidas.")

"""## BAIRRO"""

df['BAIRRO'] = df['BAIRRO'].str.upper()
df['BAIRRO'] = df['BAIRRO'].str.strip()
unique_bairros = df['BAIRRO'].unique()
unique_bairros_sorted = sorted(unique_bairros)
print(unique_bairros_sorted)
num_unique_bairros = df['BAIRRO'].nunique()
print(num_unique_bairros)

# Cria um novo dataframe filtrado
df_filtered = df[df.groupby('BAIRRO')['BAIRRO'].transform('count') > 50]

# Calcula a ordem das categorias com base na frequência
order = df_filtered['BAIRRO'].value_counts().index

# Cria o gráfico de contagem para a coluna 'BAIRRO' com os bairros com maior número de vendas
sns.countplot(x='BAIRRO', data=df_filtered, order=order)
plt.xticks(rotation=90)  # Rotaciona os nomes dos bairros no eixo x para melhor visualização
plt.show()

print(unique_bairros_sorted)

"""## CIDADE"""

df['CIDADE'] = df['CIDADE'].str.upper()
df['CIDADE'] = df['CIDADE'].str.strip()
unique_cidades = df['CIDADE'].unique()
unique_cidades_sorted = sorted(unique_cidades)
print(unique_cidades_sorted)
num_unique_cidades = df['CIDADE'].nunique()
print(num_unique_cidades)

# Cria um novo dataframe filtradando cidades que aparecem mais de 1 vezes
df_principais_cidade = df[df.groupby('CIDADE')['CIDADE'].transform('count') > 1]

# Cria o gráfico de contagem
sns.countplot(x='CIDADE', data=df_principais_cidade)
plt.xticks(rotation=90)  # Rotaciona os nomes das cidades no eixo x para melhor visualização
plt.show()

"""## UF"""

sns.countplot(x= df ['UF'])

def statistic(_dataset):
  """
  DOCSTRING:
  Describe (without count) + Insert range: max - min
  """
  _describe = _dataset.describe()
  _describe = _describe.drop('count')
  _size = _describe.shape
  _describe.loc['range'] = _describe['max'] - _describe['min']
  _describe.loc['mode'] = stats.mode(_dataset, keepdims = False)[0]
  _describe = np.around(_describe.sort_values(), 2)
  return _describe

def PlotarStatistic(dataset, yLabel = None):

  _describe = statistic(dataset)

  # Símbolos da estatística estimada da população
  mean_mu = 'MÉDIA:  $\\overline{\mu}$ = '
  mean_mu += f'{_describe["mean"]: .2f}'

  median_mu = 'MEDIANA: $\\tilde{\mu}$ = '
  median_mu += f'{_describe["50%"]: .2f}'

  mode_mu = 'MODA: $\\hat{\mu}$ = '
  mode_mu += f'{_describe["mode"]: .2f}'

  enter = '\n'

  label = mean_mu + enter + median_mu + enter + mode_mu
  dataset.agg(statistic).plot(figsize = [15, 6], label='MEDIDAS DE RESUMO: ')
  plt.plot(list(_describe.index).index('mean'), _describe["mean"], 'D', label=mean_mu)
  plt.plot(list(_describe.index).index('50%'), _describe["50%"], 'D', label=median_mu)
  plt.plot(list(_describe.index).index('mode'), _describe["mode"], 'D', label=mode_mu)

  plt.title(f'ESTATÍSTICA DESCRITIVA DO DATASET')
  plt.legend()
  plt.ylabel(yLabel)
  plt.show()
  print('ESTATÍSTICA DESCRITIVA DO DATASET:')
  print(_describe)

from scipy import stats
df['SUBTOTAL'].describe()
_describe = statistic(df['SUBTOTAL'])
_describe
PlotarStatistic(df['SUBTOTAL'], "SUBTOTAL")

"""## WEB SCRAPING"""

import requests
from bs4 import BeautifulSoup

# Cria listas vazias para armazenar os nomes e preços dos produtos
product_names = []
product_prices = []

# Varre todas as 21 páginas
for i in range(1, 22):
    url = f"https://www.tavipapelaria.com.br/categoria-produto/papelaria/page/{i}"

    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    products = soup.select('a.product-name')
    prices = soup.select('div.product-price')

    for product, price in zip(products, prices):
        product_name = product.text.strip()  # Limpa espaços em branco no começo/fim do texto
        product_price = price.text.strip()

        # Verifica se há um preço de promoção
        if '\n' in product_price:
            product_price = product_price.split('\n')[1]

        # Assume que o preço está no formato "R$ 9,99"
        # Remove o "R$ " e troca a vírgula por ponto
        product_price = product_price.replace("R$ ", "").replace(",", ".")
        product_price = float(product_price)  # Converte o preço para float

        # Adiciona o nome e o preço do produto às listas
        product_names.append(product_name)
        product_prices.append(product_price)

# Cria um DataFrame com as listas de nomes e preços dos produtos
df_concorrente = pd.DataFrame({
    "Produto": product_names,
    "Preço": product_prices
})

# Exibe o DataFrame
print(df_concorrente)

for product in df['NOME PRODUTO'].unique():
    # Verificar se algum produto no df_concorrente contém o nome do produto
    similar_products = df_concorrente['Produto'].str.contains(product, case=False, na=False)

    if similar_products.sum() > 0:
        # Calcula o preço médio dos produtos similares
        avg_price = df_concorrente[similar_products]['Preço'].mean()

        print(f'Produto: {product}, Preço médio na concorrência: {avg_price:.2f}')
    else:
        print(f'Produto: {product}, Sem produtos similares encontrados')

base_url = "https://www.tavipapelaria.com.br/loja/busca.php?loja=586385&palavra_busca="
product_names = df['NOME PRODUTO'].unique()
products = []

# Para cada produto único no seu dataframe
for product in product_names:
    # Crie a URL de pesquisa
    search_url = base_url + product

    # Faça uma solicitação GET à URL de pesquisa
    response = requests.get(search_url)

    # Se a solicitação foi bem sucedida
    if response.status_code == 200:
        # Analise o HTML da página de resultados
        soup = BeautifulSoup(response.content, 'html.parser')

        # Encontre o primeiro produto nos resultados
        product_result = soup.find('a', class_='product-name')
        price_result = soup.find('div', class_='product-price')

        if product_result and price_result:
            product_name = product_result.text

            # Substitua caracteres não numéricos e selecione o primeiro preço
            product_price = price_result.text.replace('R$', '').replace(',', '.').strip().split('\n')[0]

            # Tente converter o preço para float
            try:
                product_price = float(product_price)
            except ValueError:
                product_price = None

            products.append({
                'Produto': product_name,
                'Preço': product_price
            })

# Crie um dataframe com os resultados
df_concorrente_busca = pd.DataFrame(products)
df_concorrente_busca

# Criar um novo dataframe agrupando 'NOME PRODUTO' e obtendo a média de 'VALOR BRUTO VENDA'
df_produto_preco = df.groupby('NOME PRODUTO')['VALOR BRUTO VENDA'].mean().reset_index()

# Arredondar a coluna 'Preço' para duas casas decimais
df_produto_preco['VALOR BRUTO VENDA'] = df_produto_preco['VALOR BRUTO VENDA'].round(2)

# Renomear colunas para facilitar a junção com o df_concorrente
df_produto_preco.columns = ['Produto', 'Preço']

df_produto_preco

print(df['NOME PRODUTO'].unique())
print(df_concorrente_busca['Produto'].unique())

mapeamento_produtos = {
    'CANETA HIDROCOR 12 CORES NEO-PEN G/G': 'Caneta Hidrográfica Neo-Pen Gigante 12 Cores Compactor',
    'LAPIS ECOLAPIS GRIP2001 CORES': 'Lápis de Cor 12 Cores Ecolápis Sextavado Multicolor',
    'CANETA ESF BIC CRISTAL': 'Caneta Esferográfica Cristal Média 1.0mm Vermelha Bic',
    'CARTOLINA COMUM CORES': 'Cartolina 500 X 660mm Branco 120g Millennium',
    'EVA 40 X 50 CM DECORADO': 'EVA Listrado 400mm x 500mm Brasil 1.5mm BRW',
    'PAPEL CENARIO FOLHA 66X96 BRANCO/KRAFT/OURO/NATURAL': 'Papel Kraft A4 Natural 180g 50 Folhas Usapel',
    'PAPEL CARTAO CORES 50X66': 'Pasta Porta Cartão Chies Azul Royal p/ 400 Cartão',
    'LAPIS P/DESENHO 2B': 'Lapis Grafite Sextavado Preto 2B Leonora',
    'LAPIS P/DESENHO 6B': 'Lápis de Cor Vibes Amazonia 12 cores + 1 lápis 6B Tris',
    'COLA BRANCA 110G TENAZ': 'Cola Liquida 110g Tenaz Pritt',
    'CONTACT COLORIDO 1M (CORES BASICAS)': 'Papel Contact Vermelho 45cm x 10m Polifix',
    'PAPEL A4 C/500FL CHAMEX OFFICE 75G': 'Papel Sulfite A4 Colorido 75g 100 Folhas Jandaia',
    'EVA 40 X 50 CM CORES': 'EVA Listrado 400mm x 500mm Brasil 1.5mm BRW',
    'MASSA DE MODELAR 12 CORES SOFT 180G': 'Massinha de Modelar 12 Cores 180g Soft Acrilex',
    'COLA BRANCA 90G MERCUR': 'Cola Branca Lavável 90g Mercur',
    'LAPIS DE COR 12 CORES ECOLAPIS 120112+2N': 'Lápis de Cor 12 Cores Ecolápis Sextavado Multicolor',
    'BORRACHA TK FABER BRANCA (OF/7024N)': 'Borracha Branca Pequena com Capa TK Faber Castell',
    'CANETA ESF SPIRO 0.7 PRETO': 'Caneta Esferográfica Needle Preto 0.7 mm Bic'
}

# Aplicar o mapeamento no dataframe 'df_produto_preco'
df_produto_preco['Produto'] = df_produto_preco['Produto'].map(mapeamento_produtos)

"""# Tratamento dos atributos"""

df.isnull().sum()

"""# Regressão Linear"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score


# Definindo variáveis independentes e dependentes
X = df[['QUANTIDADE VENDIDA']] # exemplo com uma variável independente
y = df['SUBTOTAL']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predição
y_pred = regressor.predict(X_test)

# Calculando o erro
mse = mean_squared_error(y_test, y_pred)
print(f'MSE: {mse}')

r2 = r2_score(y_test, y_pred)
print(r2)

"""Análise residual"""

df.loc[X_train.index, 'VALOR_PREDITO'] = regressor.predict(X_train)
df.loc[X_train.index, 'RESIDUAL'] = y_train - df.loc[X_train.index, 'VALOR_PREDITO']
sns.jointplot(x='VALOR_PREDITO', y='RESIDUAL', data=df, hue='PESSOA/EMPRESA')
plt.show()

"""Estabelecendo a escala para avaliar a qualidade do MSE"""

min_value = df['SUBTOTAL'].min()
max_value = df['SUBTOTAL'].max()

print(f"Valor mínimo: {min_value}")
print(f"Valor máximo: {max_value}")

"""Validação Cruzada"""

from sklearn.model_selection import cross_val_score

# Definindo variáveis independentes e dependentes
X = df[['QUANTIDADE VENDIDA']] # exemplo com uma variável independente
y = df['SUBTOTAL']

# Instanciando o modelo
regressor = LinearRegression()

# Realizando a validação cruzada com 10 folds
scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')

# Como os scores são negativos (convenção do scikit-learn), vamos multiplicar por -1
mse_scores = -scores

# Calculando a média e o desvio padrão do MSE
mean_mse = mse_scores.mean()
std_mse = mse_scores.std()

print(f'Média do MSE: {mean_mse}')
print(f'Desvio Padrão do MSE: {std_mse}')

from sklearn.linear_model import BayesianRidge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import SVR

reg1 = BayesianRidge(tol=1e-6, fit_intercept=False, compute_score=True)

rng = np.random.RandomState(1)
reg2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng)

reg3 = SVR(kernel="poly", C=100, gamma="auto", degree=3, epsilon=0.1, coef0=1)

scores1 = cross_val_score(reg1, X, y, cv=10)
print("Regressor 1 (BayesianRidge) - Média das pontuações de validação cruzada:", scores1.mean())

scores2 = cross_val_score(reg2, X, y, cv=10)
print("Regressor 2 (AdaBoostRegressor) - Média das pontuações de validação cruzada:", scores2.mean())

scores3 = cross_val_score(reg3, X, y, cv=10)
print("Regressor 3 (SVR) - Média das pontuações de validação cruzada:", scores3.mean())

"""#Curva do cotovelo e Custerizador"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(df[['VALOR BRUTO VENDA', 'QUANTIDADE VENDIDA']])
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Curva do Cotovelo')
plt.xlabel('Número de Clusters')
plt.ylabel('WCSS')
plt.show()

from sklearn.preprocessing import StandardScaler

# Normalizar os dados antes de aplicar o K-Means
scaler = StandardScaler()
df_normalized = scaler.fit_transform(df[['SUBTOTAL']])

# Aplicação do K-Means para 2 clusters
kmeans = KMeans(n_clusters=2, random_state=0)
df['CLUSTER'] = kmeans.fit_predict(df_normalized)

# Contagem do número de amostras em cada cluster
contagem_clusters = df['CLUSTER'].value_counts()

print(contagem_clusters)

""""Excluindo" outliers"""

Q1 = df['SUBTOTAL'].quantile(0.25)
Q3 = df['SUBTOTAL'].quantile(0.75)
IQR = Q3 - Q1

limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

# Filtrar as linhas que não contêm outliers
df_sem_outliers = df[(df['SUBTOTAL'] >= limite_inferior) & (df['SUBTOTAL'] <= limite_superior)]

"""Testando o df_sem_outliers"""

# Normalizar os dados antes de aplicar o K-Means
scaler = StandardScaler()
df_normalized = scaler.fit_transform(df_sem_outliers[['SUBTOTAL']])
kmeans = KMeans(n_clusters=5, n_init=20)
df_sem_outliers['CLUSTER'] = kmeans.fit_predict(df_normalized)
contagem_clusters = df_sem_outliers['CLUSTER'].value_counts()

print(contagem_clusters)

from sklearn.model_selection import cross_val_score

# Definindo variáveis independentes e dependentes
X = df_sem_outliers[['QUANTIDADE VENDIDA']] # exemplo com uma variável independente
y = df_sem_outliers['SUBTOTAL']

# Instanciando o modelo
regressor = LinearRegression()

# Realizando a validação cruzada com 10 folds
scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')

# Como os scores são negativos (convenção do scikit-learn), vamos multiplicar por -1
mse_scores = -scores

# Calculando a média e o desvio padrão do MSE
mean_mse = mse_scores.mean()
std_mse = mse_scores.std()

print(f'Média do MSE: {mean_mse}')
print(f'Desvio Padrão do MSE: {std_mse}')

min_value = df_sem_outliers['SUBTOTAL'].min()
max_value = df_sem_outliers['SUBTOTAL'].max()

print(f"Valor mínimo: {min_value}")
print(f"Valor máximo: {max_value}")

from sklearn.cluster import DBSCAN

# Normalizando os dados (importante para algoritmos baseados em distância)
dados_normalizados = StandardScaler().fit_transform(df['SUBTOTAL'].values.reshape(-1, 1))

# Aplicando DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # Ajuste os valores de eps e min_samples conforme necessário
df['cluster'] = dbscan.fit_predict(dados_normalizados)

print(df['cluster'].value_counts())

from sklearn.neighbors import NearestNeighbors

# Considerando que seus dados estão na coluna 'SUBTOTAL'
dados = df['SUBTOTAL'].values.reshape(-1, 1)

# Defina o número de vizinhos que você deseja (k)
k = 5
neigh = NearestNeighbors(n_neighbors=k)
nbrs = neigh.fit(dados)
distances, indices = nbrs.kneighbors(dados)

# Ordene as distâncias
distances = np.sort(distances, axis=0)
distances = distances[:,1]

plt.figure(figsize=(10,5))
plt.plot(distances)
plt.title('Gráfico de Distância k')
plt.xlabel('Pontos')
plt.ylabel(f'Distância para o {k}° vizinho mais próximo')
plt.show()

df.loc[X_test.index, 'GASTO ESTIMADO'] = y_pred
print(df[['NOME CLIENTE', 'GASTO ESTIMADO']])